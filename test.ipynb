{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import set_up_data_loader, MultimodalConfig\n",
    "from train_utils import *\n",
    "import prompt_based.distilbert\n",
    "import normal.bert\n",
    "\n",
    "# from transformers.optimization import AdamW\n",
    "from globals import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahul\\Desktop\\nlp-proj\\data_utils.py:127: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  all_visual = torch.tensor([f.visual for f in features], dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    train_data_loader,\n",
    "    dev_data_loader,\n",
    "    test_data_loader,\n",
    "    num_train_optimization_steps,\n",
    ") = set_up_data_loader('mosi.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing AVPrompt_DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing AVPrompt_DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AVPrompt_DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AVPrompt_DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'distilbert.mag.LayerNorm.weight', 'distilbert.mag.LayerNorm.bias', 'pre_classifier.weight', 'distilbert.mag.W_ha.weight', 'pre_classifier.bias', 'distilbert.mag.W_hv.bias', 'distilbert.mag.W_hv.weight', 'distilbert.prompt.LayerNorm.bias', 'classifier.weight', 'distilbert.mag.W_v.weight', 'distilbert.prompt.position_embeddings.weight', 'distilbert.prompt.LayerNorm.weight', 'distilbert.mag.W_a.weight', 'distilbert.mag.W_ha.bias', 'distilbert.mag.W_v.bias', 'distilbert.mag.W_a.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MAG with beta_shift:1.0 hidden_prob:0.5\n",
      "epoch:0, train_loss:2.3093919492349393 valid_loss: 2.7613794803619385, test: 1.5060870837619287\n",
      "epoch:1, train_loss:2.318026757821804 valid_loss: 2.738840937614441, test: 1.4984339870395182\n",
      "epoch:2, train_loss:2.2118003630056613 valid_loss: 2.7232396602630615, test: 1.499301182687103\n",
      "epoch:3, train_loss:1.9561825888912852 valid_loss: 1.991182565689087, test: 1.360209542510761\n",
      "epoch:4, train_loss:1.2735856793275693 valid_loss: 1.3434696793556213, test: 0.9114820737242414\n",
      "epoch:5, train_loss:0.9004731127401677 valid_loss: 1.2509133219718933, test: 0.8919220841902074\n",
      "epoch:6, train_loss:0.7241205442242507 valid_loss: 1.1562008261680603, test: 0.8523830776262801\n",
      "epoch:7, train_loss:0.5824292265787357 valid_loss: 1.1581212282180786, test: 0.8556351222161858\n",
      "epoch:8, train_loss:0.49296029057444596 valid_loss: 1.1693871021270752, test: 0.7936634340366164\n",
      "epoch:9, train_loss:0.41472737999950965 valid_loss: 1.2048977315425873, test: 0.8020775148175827\n",
      "epoch:10, train_loss:0.3758934870271421 valid_loss: 1.198676347732544, test: 0.7929951214608345\n",
      "epoch:11, train_loss:0.31172144994503115 valid_loss: 1.1615606844425201, test: 0.8628898831610461\n",
      "epoch:12, train_loss:0.30487392988146805 valid_loss: 1.3245929479599, test: 0.7697565333505396\n",
      "epoch:13, train_loss:0.28227913833972884 valid_loss: 1.2804849743843079, test: 0.764125482848957\n",
      "epoch:14, train_loss:0.23082398532367335 valid_loss: 1.2830672860145569, test: 0.7842002589497289\n",
      "epoch:15, train_loss:0.21681051083454272 valid_loss: 1.2502031326293945, test: 0.8060979347647601\n",
      "epoch:16, train_loss:0.21362246645659935 valid_loss: 1.2931830286979675, test: 0.7814696687231025\n",
      "epoch:17, train_loss:0.19662195684888015 valid_loss: 1.2618026733398438, test: 0.7931036553214098\n",
      "epoch:18, train_loss:0.19969191402196884 valid_loss: 1.238175868988037, test: 0.7874586670403546\n",
      "epoch:19, train_loss:0.17186781146177432 valid_loss: 1.2710682153701782, test: 0.7804939346427083\n",
      "epoch:20, train_loss:0.16945989575327897 valid_loss: 1.2540929317474365, test: 0.7854546863237353\n",
      "epoch:21, train_loss:0.16309410624387788 valid_loss: 1.3056498765945435, test: 0.7907705665829072\n",
      "epoch:22, train_loss:0.1749595563586165 valid_loss: 1.261902391910553, test: 0.8032459353957679\n",
      "epoch:23, train_loss:0.15717762690491793 valid_loss: 1.2546467185020447, test: 0.8020646577850269\n",
      "epoch:24, train_loss:0.14542597868457074 valid_loss: 1.2891556024551392, test: 0.7914179564128299\n",
      "epoch:25, train_loss:0.150911795838577 valid_loss: 1.2248912453651428, test: 0.7888420735812881\n",
      "epoch:26, train_loss:0.14765876813269244 valid_loss: 1.2443512678146362, test: 0.7968825036083093\n",
      "epoch:27, train_loss:0.17105075280840804 valid_loss: 1.2690829038619995, test: 0.7971433730803533\n",
      "epoch:28, train_loss:0.14335849790311442 valid_loss: 1.2644096612930298, test: 0.7951165812047383\n",
      "epoch:29, train_loss:0.13704780861735344 valid_loss: 1.247433841228485, test: 0.7936734730377794\n"
     ]
    }
   ],
   "source": [
    "multimodal_config = MultimodalConfig(1.0, 0.5)\n",
    "model = prompt_based.distilbert.AVPrompt_DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", multimodal_config=multimodal_config, num_labels=1,\n",
    ")\n",
    "\n",
    "# frozen_modules = [model.bert.embeddings, model.bert.encoder]\n",
    "# for module in frozen_modules:\n",
    "#     for param in module.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "model, optimizer, scheduler = prep_for_training(model, num_train_optimization_steps, learning_rate=1e-5)\n",
    "train(model, train_data_loader, dev_data_loader, test_data_loader, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8320610687022901,\n",
       " 0.7936734843984928,\n",
       " 0.7621109146368132,\n",
       " 0.8326025503205094)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_model(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MAG_BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing MAG_BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MAG_BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MAG with beta_shift:1.0 hidden_prob:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MAG_BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.mag.LayerNorm.bias', 'bert.mag.W_v.weight', 'bert.mag.W_a.weight', 'classifier.bias', 'bert.mag.W_a.bias', 'bert.mag.LayerNorm.weight', 'bert.mag.W_ha.bias', 'classifier.weight', 'bert.mag.W_ha.weight', 'bert.mag.W_hv.bias', 'bert.mag.W_hv.weight', 'bert.mag.beta_shift', 'bert.mag.W_v.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:2.418791925034872 valid_loss: 2.7159016132354736, test: 1.4846515943257153\n",
      "epoch:1, train_loss:2.293390271140308 valid_loss: 2.550636887550354, test: 1.4541818938870783\n",
      "epoch:2, train_loss:2.0612785583589135 valid_loss: 2.022863268852234, test: 1.3002059629123004\n",
      "epoch:3, train_loss:1.5381339672135144 valid_loss: 1.3195209503173828, test: 1.0438050266214867\n",
      "epoch:4, train_loss:1.1568143624903224 valid_loss: 1.3156803250312805, test: 1.0020405007295226\n",
      "epoch:5, train_loss:1.1075649465002664 valid_loss: 1.2095701098442078, test: 0.8945106363928172\n",
      "epoch:6, train_loss:0.9391411359353763 valid_loss: 1.2157057523727417, test: 0.9714400216005743\n",
      "epoch:7, train_loss:0.8001305375747928 valid_loss: 1.1936660408973694, test: 0.8634026549692543\n",
      "epoch:8, train_loss:0.7464494992320131 valid_loss: 1.1524189710617065, test: 0.843613120783938\n",
      "epoch:9, train_loss:0.7016445933443654 valid_loss: 1.152364432811737, test: 0.8491224108746841\n",
      "epoch:10, train_loss:0.6636516884787054 valid_loss: 1.1681677103042603, test: 0.8750903693831149\n",
      "epoch:11, train_loss:0.732958082018829 valid_loss: 1.193803310394287, test: 0.8800182642429385\n",
      "epoch:12, train_loss:0.604863447387044 valid_loss: 1.211556613445282, test: 0.8555816157550359\n",
      "epoch:13, train_loss:0.676644727224257 valid_loss: 1.1657941341400146, test: 0.8051452564004495\n",
      "epoch:14, train_loss:0.5420993123112655 valid_loss: 1.1706852912902832, test: 0.7842363293613271\n",
      "epoch:15, train_loss:0.5288958387767396 valid_loss: 1.1894928812980652, test: 0.8346309261332543\n",
      "epoch:16, train_loss:0.4798558565174661 valid_loss: 1.150567352771759, test: 0.8121207938735722\n",
      "epoch:17, train_loss:0.45439771290232495 valid_loss: 1.178904414176941, test: 0.8049584943786946\n",
      "epoch:18, train_loss:0.45176306512297654 valid_loss: 1.1637039184570312, test: 0.8114662040338695\n",
      "epoch:19, train_loss:0.440551893921887 valid_loss: 1.1638484001159668, test: 0.8176659345463316\n",
      "epoch:20, train_loss:0.3922502705600204 valid_loss: 1.1674732565879822, test: 0.7985272811228082\n",
      "epoch:21, train_loss:0.3980381912665396 valid_loss: 1.1427077949047089, test: 0.8174086275048383\n",
      "epoch:22, train_loss:0.40489218947364064 valid_loss: 1.1688819527626038, test: 0.824017530628249\n",
      "epoch:23, train_loss:0.3990300072402489 valid_loss: 1.1836548447608948, test: 0.8039119515194778\n",
      "epoch:24, train_loss:0.39014631062291744 valid_loss: 1.1697116494178772, test: 0.788813226795151\n",
      "epoch:25, train_loss:0.39288208760866306 valid_loss: 1.1726157665252686, test: 0.7986978865633826\n",
      "epoch:26, train_loss:0.3815160731716854 valid_loss: 1.148974061012268, test: 0.7900193796187633\n",
      "epoch:27, train_loss:0.39928946589551323 valid_loss: 1.1829939782619476, test: 0.7910825511838757\n",
      "epoch:28, train_loss:0.3556923466484721 valid_loss: 1.1455907821655273, test: 0.7964718975866114\n",
      "epoch:29, train_loss:0.349661202692404 valid_loss: 1.1904529333114624, test: 0.7959383140652234\n"
     ]
    }
   ],
   "source": [
    "multimodal_config = MultimodalConfig(1.0, 0.5)\n",
    "model = normal.bert.MAG_BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', multimodal_config=multimodal_config, num_labels=1,\n",
    ")\n",
    "\n",
    "# frozen_modules = [model.distilbert.embeddings, model.distilbert.transformer]\n",
    "# for module in frozen_modules:\n",
    "#     for param in module.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "model, optimizer, scheduler = prep_for_training(model, num_train_optimization_steps, learning_rate=1e-5)\n",
    "train(model, train_data_loader, dev_data_loader, test_data_loader, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8244274809160306, 0.7959383508460667, 0.777387463091041, 0.8250752226564783)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_model(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "976d43950396151e7cab9926e73b5abe20542c9ce0b57c070f05867d690adfc3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tabular')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
