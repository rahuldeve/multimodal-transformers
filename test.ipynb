{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import set_up_data_loader, MultimodalConfig\n",
    "# from bert import MAG_BertForSequenceClassification\n",
    "# from miniLM import MAG_BertForSequenceClassification\n",
    "# import prompt_based.minilm.minilm\n",
    "import prompt_based.distilbert\n",
    "import normal.bert\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers.optimization import AdamW\n",
    "from globals import *\n",
    "\n",
    "# import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_for_training(model, num_train_optimization_steps: int, learning_rate):\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Prepare optimizer\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = Adam(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_proportion * num_train_optimization_steps,\n",
    "        num_training_steps=num_train_optimization_steps,\n",
    "    )\n",
    "    # scheduler = None\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataloader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "        input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "        visual = torch.squeeze(visual, 1)\n",
    "        acoustic = torch.squeeze(acoustic, 1)\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            visual,\n",
    "            acoustic,\n",
    "            # token_type_ids=segment_ids,\n",
    "            attention_mask=input_mask,\n",
    "            labels=None,\n",
    "        )\n",
    "        logits = outputs[0]\n",
    "        loss_fct = MSELoss()\n",
    "        loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "        if gradient_accumulation_step > 1:\n",
    "            loss = loss / gradient_accumulation_step\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_step == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    return tr_loss / nb_tr_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, dev_dataloader, optimizer):\n",
    "    model.eval()\n",
    "    dev_loss = 0\n",
    "    nb_dev_examples, nb_dev_steps = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(dev_dataloader):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            # print(segment_ids)\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                # token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None,\n",
    "            )\n",
    "            logits = outputs[0]\n",
    "\n",
    "            loss_fct = MSELoss()\n",
    "            loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "            if gradient_accumulation_step > 1:\n",
    "                loss = loss / gradient_accumulation_step\n",
    "\n",
    "            dev_loss += loss.item()\n",
    "            nb_dev_steps += 1\n",
    "\n",
    "    return dev_loss / nb_dev_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score_model(model, test_dataloader, use_zero=False):\n",
    "\n",
    "    preds, y_test = test_epoch(model, test_dataloader)\n",
    "    non_zeros = np.array(\n",
    "        [i for i, e in enumerate(y_test) if e != 0 or use_zero])\n",
    "\n",
    "    preds = preds[non_zeros]\n",
    "    y_test = y_test[non_zeros]\n",
    "\n",
    "    mae = np.mean(np.absolute(preds - y_test))\n",
    "    corr = np.corrcoef(preds, y_test)[0][1]\n",
    "\n",
    "    preds = preds >= 0\n",
    "    y_test = y_test >= 0\n",
    "\n",
    "    f_score = f1_score(y_test, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "\n",
    "    return acc, mae, corr, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, test_dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                # token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None,\n",
    "            )\n",
    "\n",
    "            logits = outputs[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "            logits = np.squeeze(logits).tolist()\n",
    "            label_ids = np.squeeze(label_ids).tolist()\n",
    "\n",
    "            preds.extend(logits)\n",
    "            labels.extend(label_ids)\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    test_data_loader,\n",
    "    optimizer,\n",
    "    scheduler\n",
    "):\n",
    "    # valid_losses = []\n",
    "    # test_accuracies = []\n",
    "\n",
    "    for epoch_i in range(int(n_epochs)):\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler)\n",
    "        valid_loss = eval_epoch(model, validation_dataloader, optimizer)\n",
    "        test_acc, test_mae, test_corr, test_f_score = test_score_model(\n",
    "            model, test_data_loader\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"epoch:{}, train_loss:{} valid_loss: {}, test: {}\".format(\n",
    "                epoch_i, train_loss, valid_loss, test_mae\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahul\\Desktop\\nlp-proj\\data_utils.py:127: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  all_visual = torch.tensor([f.visual for f in features], dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    train_data_loader,\n",
    "    dev_data_loader,\n",
    "    test_data_loader,\n",
    "    num_train_optimization_steps,\n",
    ") = set_up_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing AVPrompt_DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing AVPrompt_DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AVPrompt_DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AVPrompt_DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.mag.LayerNorm.weight', 'distilbert.prompt.LayerNorm.weight', 'distilbert.mag.W_ha.weight', 'distilbert.mag.W_ha.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'distilbert.mag.W_v.weight', 'distilbert.prompt.LayerNorm.bias', 'distilbert.mag.W_hv.bias', 'distilbert.mag.W_a.bias', 'distilbert.mag.W_hv.weight', 'distilbert.prompt.position_embeddings.weight', 'classifier.bias', 'distilbert.mag.LayerNorm.bias', 'distilbert.mag.W_v.bias', 'distilbert.mag.W_a.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MAG with beta_shift:1.0 hidden_prob:0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rahul\\Desktop\\nlp-proj\\test.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000008?line=5'>6</a>\u001b[0m \u001b[39m# frozen_modules = [model.bert.embeddings, model.bert.encoder]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000008?line=6'>7</a>\u001b[0m \u001b[39m# for module in frozen_modules:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000008?line=7'>8</a>\u001b[0m \u001b[39m#     for param in module.parameters():\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000008?line=8'>9</a>\u001b[0m \u001b[39m#         param.requires_grad = False\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000008?line=10'>11</a>\u001b[0m model, optimizer, scheduler \u001b[39m=\u001b[39m prep_for_training(model, num_train_optimization_steps, learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000008?line=11'>12</a>\u001b[0m train(model, train_data_loader, dev_data_loader, test_data_loader, optimizer, scheduler)\n",
      "\u001b[1;32mc:\\Users\\rahul\\Desktop\\nlp-proj\\test.ipynb Cell 7'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, validation_dataloader, test_data_loader, optimizer, scheduler)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000006?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000006?line=1'>2</a>\u001b[0m     model,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000006?line=2'>3</a>\u001b[0m     train_dataloader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000006?line=8'>9</a>\u001b[0m     \u001b[39m# valid_losses = []\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000006?line=9'>10</a>\u001b[0m     \u001b[39m# test_accuracies = []\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000006?line=11'>12</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch_i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(n_epochs)):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000006?line=12'>13</a>\u001b[0m         train_loss \u001b[39m=\u001b[39m train_epoch(model, train_dataloader, optimizer, scheduler)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000006?line=13'>14</a>\u001b[0m         valid_loss \u001b[39m=\u001b[39m eval_epoch(model, validation_dataloader, optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000006?line=14'>15</a>\u001b[0m         test_acc, test_mae, test_corr, test_f_score \u001b[39m=\u001b[39m test_score_model(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000006?line=15'>16</a>\u001b[0m             model, test_data_loader\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000006?line=16'>17</a>\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\rahul\\Desktop\\nlp-proj\\test.ipynb Cell 3'\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_dataloader, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000002?line=22'>23</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m gradient_accumulation_step\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000002?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000002?line=26'>27</a>\u001b[0m tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000002?line=27'>28</a>\u001b[0m nb_tr_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahul/Desktop/nlp-proj/test.ipynb#ch0000002?line=29'>30</a>\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m gradient_accumulation_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "multimodal_config = MultimodalConfig(1.0, 0.5)\n",
    "model = prompt_based.distilbert.AVPrompt_DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", multimodal_config=multimodal_config, num_labels=1,\n",
    ")\n",
    "\n",
    "# frozen_modules = [model.bert.embeddings, model.bert.encoder]\n",
    "# for module in frozen_modules:\n",
    "#     for param in module.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "model, optimizer, scheduler = prep_for_training(model, num_train_optimization_steps, learning_rate=1e-2)\n",
    "train(model, train_data_loader, dev_data_loader, test_data_loader, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8229007633587786,\n",
       " 0.8649354306810344,\n",
       " 0.7418325494026404,\n",
       " 0.8230645190397269)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_model(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MAG_MiniLMForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['mag.W_a.bias', 'mag.W_a.weight', 'mag.LayerNorm.bias', 'mag.beta_shift', 'mag.W_ha.bias', 'classifier.weight', 'mag.W_v.weight', 'mag.W_v.bias', 'mag.LayerNorm.weight', 'classifier.bias', 'mag.W_ha.weight', 'mag.W_hv.bias', 'mag.W_hv.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MAG with beta_shift:1.0 hidden_prob:0.5\n",
      "epoch:0, train_loss:2.319864531842674 valid_loss: 2.7820749282836914, test: 1.4851906323165611\n",
      "epoch:1, train_loss:2.4142271687344805 valid_loss: 2.7926435470581055, test: 1.49044872877252\n",
      "epoch:2, train_loss:2.2572515912172273 valid_loss: 2.7407796382904053, test: 1.5007158292955114\n",
      "epoch:3, train_loss:2.4387300479702834 valid_loss: 2.7242801189422607, test: 1.507470249083206\n",
      "epoch:4, train_loss:2.306977463931572 valid_loss: 2.7018152475357056, test: 1.506412047466249\n",
      "epoch:5, train_loss:2.275074072000457 valid_loss: 2.705891489982605, test: 1.5132590400126145\n",
      "epoch:6, train_loss:2.2081837137116165 valid_loss: 2.710377812385559, test: 1.5169823122388535\n",
      "epoch:7, train_loss:2.3671285873506127 valid_loss: 2.6926037073135376, test: 1.5130032903365507\n",
      "epoch:8, train_loss:2.2341104774940304 valid_loss: 2.7082098722457886, test: 1.50752694860442\n",
      "epoch:9, train_loss:2.1957710603388345 valid_loss: 2.627762794494629, test: 1.4893595833052304\n",
      "epoch:10, train_loss:2.042109835438612 valid_loss: 2.4587860107421875, test: 1.4400514907545308\n",
      "epoch:11, train_loss:1.8706230274060878 valid_loss: 2.267854928970337, test: 1.387310400198548\n",
      "epoch:12, train_loss:1.7670803753341116 valid_loss: 1.9938725233078003, test: 1.2566116812673596\n",
      "epoch:13, train_loss:1.620904035684539 valid_loss: 1.7905701994895935, test: 1.1938118183565902\n",
      "epoch:14, train_loss:1.5495609917291782 valid_loss: 1.713071346282959, test: 1.1559198407095481\n",
      "epoch:15, train_loss:1.459233629994276 valid_loss: 1.6100160479545593, test: 1.1215735321880482\n",
      "epoch:16, train_loss:1.4576849021562717 valid_loss: 1.5614926218986511, test: 1.1021870958668585\n",
      "epoch:17, train_loss:1.3402744960494157 valid_loss: 1.6158789992332458, test: 1.1202576975828704\n",
      "epoch:18, train_loss:1.391132066889507 valid_loss: 1.5530586242675781, test: 1.0630812669343508\n",
      "epoch:19, train_loss:1.2708915835473595 valid_loss: 1.5299423933029175, test: 1.0359196040295917\n",
      "epoch:20, train_loss:1.3368090565611677 valid_loss: 1.505990207195282, test: 1.0637798252206239\n",
      "epoch:21, train_loss:1.3171744441113822 valid_loss: 1.4827191233634949, test: 1.0428057272463525\n",
      "epoch:22, train_loss:1.2932812821051878 valid_loss: 1.479587972164154, test: 1.0172339919698603\n",
      "epoch:23, train_loss:1.2701459846845486 valid_loss: 1.462007761001587, test: 1.0142274899331796\n",
      "epoch:24, train_loss:1.3197513266307552 valid_loss: 1.465827226638794, test: 1.0260635008485384\n",
      "epoch:25, train_loss:1.235458043472069 valid_loss: 1.461090624332428, test: 1.0256427160165635\n",
      "epoch:26, train_loss:1.3495625242954348 valid_loss: 1.4572480916976929, test: 1.0136976958330692\n",
      "epoch:27, train_loss:1.2595701221285798 valid_loss: 1.4709158539772034, test: 1.0233388695034078\n",
      "epoch:28, train_loss:1.2421136469375798 valid_loss: 1.4542243480682373, test: 1.0144324065349597\n",
      "epoch:29, train_loss:1.2514358482624564 valid_loss: 1.435672402381897, test: 1.012808735084272\n"
     ]
    }
   ],
   "source": [
    "multimodal_config = MultimodalConfig(1.0, 0.5)\n",
    "model = normal.bert.MAG_BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', multimodal_config=multimodal_config, num_labels=1,\n",
    ")\n",
    "\n",
    "# frozen_modules = [model.distilbert.embeddings, model.distilbert.transformer]\n",
    "# for module in frozen_modules:\n",
    "#     for param in module.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "model, optimizer, scheduler = prep_for_training(model, num_train_optimization_steps, learning_rate=1e-5)\n",
    "train(model, train_data_loader, dev_data_loader, test_data_loader, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7984732824427481,\n",
       " 1.0128087346463372,\n",
       " 0.7300915861120416,\n",
       " 0.7996784048790345)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_model(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multimodal_config = MultimodalConfig(1.0, 0.5)\n",
    "# model = distilbert.MAG_DistilBertForSequenceClassification.from_pretrained(\n",
    "#     \"distilbert-base-uncased\", multimodal_config=multimodal_config, num_labels=1,\n",
    "# )\n",
    "\n",
    "# model, optimizer, scheduler = prep_for_training(model, num_train_optimization_steps, learning_rate=1e-5)\n",
    "# train(model, train_data_loader, dev_data_loader, test_data_loader, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_score_model(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "976d43950396151e7cab9926e73b5abe20542c9ce0b57c070f05867d690adfc3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tabular')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
